{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Encoding Dictionary Definitions into Consistent Word Embeddings\n",
    "\n",
    "### Article: [bosc2018auto]\n",
    "### Repository: [github]\n",
    "\n",
    "\n",
    "[bosc2018auto]: https://research.fb.com/wp-content/uploads/2018/10/Auto-Encoding-Dictionary-Definitions-into-Consistent-Word-Embeddings.pdf\n",
    "[github]: https://github.com/tombosc/cpae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "1. Word embeddings\n",
    "\n",
    "1. Similarity vs relatedness\n",
    "\n",
    "    * Possible to optimize one vs the other, see : [artetxe2018uncovering] (not cited)\n",
    "\n",
    "1. Post-processing ==> improve existing WE \n",
    "    * [faruqui2014retrofitting]\n",
    "\n",
    "    * [wieting2016charagram] ???\n",
    "        * Character-based       \n",
    "            * Trained on paraphrase pairs from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013)\n",
    "            * looks more like Fasttext ...\n",
    "            * does not seem to address 'improvement' of WE like [faruqui2014retrofitting]\n",
    "        * pas rapp, p-e pas la bonne référence \n",
    "\n",
    "    * [mrkvsic2017semantic]: post-processing\n",
    "        * à creuser éventuellement\n",
    "\n",
    "1. (Re)Generate definition from WE ?\n",
    "    * [noraset2017definition] (code n/a)\n",
    "        * RNN\n",
    "        * WE == fusion of lexical entailment, similarity, relatedness\n",
    "\n",
    "1. Their model:\n",
    "    * Definition autoencoder\n",
    "        * LSTM to encode\n",
    "        * Decoder  \n",
    "            * ==> reconstruct BOW representation of definition\n",
    "        * Consistency penalty / recursivity\n",
    "            * ==> Proximity of ??!!\n",
    "                * WE produced by LSTM\n",
    "                * WE used by LSTM\n",
    "    * Can also improve existing WE for similarity\n",
    "                    \n",
    "\n",
    "[faruqui2014retrofitting]: https://arxiv.org/pdf/1411.4166.pdf\n",
    "[artetxe2018uncovering]: https://arxiv.org/pdf/1809.02094.pdf\n",
    "[wieting2016charagram]: https://arxiv.org/pdf/1607.02789.pdf\n",
    "[mrkvsic2017semantic]: https://arxiv.org/pdf/1706.00374.pdf\n",
    "[noraset2017definition]: https://arxiv.org/pdf/1612.00394.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model\n",
    "1. Motivation:\n",
    "    1. objective:\n",
    "        * recover definitions from WE\n",
    "    * captures more\n",
    "        * semantic semilarity (coffee and tea)\n",
    "        * instead of:\n",
    "            * relatedness (coffee and cup)\n",
    "    * single WE per word (no disambiguation in defs)\n",
    "        * single WE can capture polysemy ???\n",
    "            * [yaghoobzadeh2016intrinsic]\n",
    "\n",
    "2. Fonction de coût:\n",
    "     1. $J(\\theta', \\theta) = \\alpha J_r(\\theta', \\theta) + \\lambda J_p(\\theta)$\n",
    "\n",
    "     2. Consistency penalty :\n",
    "        1. partie $J_p$ de la fonction de coût, paramétrisé par $\\lambda$    \n",
    "\n",
    "2. Variables:\n",
    "    1. $\\mathcal{V}^{\\mathcal{D}} \\equiv$ {words used in definitions}\n",
    "    * $\\mathcal{V}^{\\mathcal{K}} \\equiv$ {words defined}\n",
    "    * $\\lambda \\equiv$  c['proximity\\_coef'] dans s2sconfigs.py\n",
    "        1. ex: = 64\n",
    "\n",
    "\n",
    "[yaghoobzadeh2016intrinsic]: https://arxiv.org/pdf/1606.07902.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"the decoder can be seen as a __conditional language model__ trained by maximum likelihood to regenerate definition $D_w$ given definition embedding $h = f_θ(D_w)$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  To summarize, the consistency penalty has several motivations.\n",
    "    1. Firstly, it deals with the fact that the recursive process of building representation of words out of definitions does not terminate __????__\n",
    "    2. Secondly, it is a way to enrich the vocabulary with new words dynamically __????__\n",
    "    1. Finally, it is a way to integrate prior knowledge in the form of pretrained embeddings __Ok__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* seq2seq:\n",
    "    * Architecture type des NMT (neural machine translator)\n",
    "    * Implantation:\n",
    "        * basé sur les Framework:\n",
    "            1. 'theano'\n",
    "            1. 'blocks': concept de 'bricks'\n",
    "                * une couche au-dessus de Theano, un peu comme Keras\n",
    "                * permet d'implanter des NN à un niveau plus 'conceptuel', cf: MLP, LSTM, ...\n",
    "            * 'fuel': gestion des données\n",
    "        * seq2seq: composé à partir de bricks :\n",
    "            1. portion encoder:\n",
    "                1. \\_main_lookup ==> blocks.bricks.lookup.LookupTable: [LookupTable], @Pas clair ....\n",
    "                    * self._main_lookup = LookupTable(self.\\_num_input_words, emb_dim, name='main_lookup')\n",
    "                1. \\_encoder_fork ==> blocks.bricks.simple.Linear: [Linear], @transformation simple, i.e.: f(x)=Wx+b\n",
    "\n",
    "                1. \\_encoder_rnn ==> blocks.bricks.recurrent.architectures.LSTM : [LSTM], @@Creuser\n",
    "            * portion Decoder:\n",
    "                1. \\_translate_layer ==> blocks.bricks.sequences.MLP [MLP]\n",
    "\n",
    "                1. \\_pre_softmax ==> blocks.bricks.simple.Linear [Linear]\n",
    "\n",
    "                1. \\_softmax ==> blocks.bricks.simple.Softmax [Softmax]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[MLP]: https://blocks.readthedocs.io/en/latest/api/bricks.html#blocks.bricks.MLP\n",
    "[Linear]: https://blocks.readthedocs.io/en/latest/api/bricks.html#blocks.bricks.Linear\n",
    "[Softmax]: https://blocks.readthedocs.io/en/latest/api/bricks.html#blocks.bricks.Softmax\n",
    "[LookupTable]: https://blocks.readthedocs.io/en/latest/api/bricks.html#blocks.bricks.lookup.LookupTable\n",
    "[Linear]: https://blocks.readthedocs.io/en/latest/api/bricks.html#blocks.bricks.Linear\n",
    "[LSTM]: https://blocks.readthedocs.io/en/latest/api/bricks.html#blocks.bricks.recurrent.architectures.LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2018-11-19 15:21:43,755: root: INFO: Cost parameters\n",
    "    ['/seq2seq/encoder_fork.W (300, 1200) trained',\n",
    "     '/seq2seq/encoder_fork.b (1200,) trained',\n",
    "     '/seq2seq/encoder_rnn.W_cell_to_forget (300,) trained',\n",
    "     '/seq2seq/encoder_rnn.W_cell_to_in (300,) trained',\n",
    "     '/seq2seq/encoder_rnn.W_cell_to_out (300,) trained',\n",
    "     '/seq2seq/encoder_rnn.W_state (300, 1200) trained',\n",
    "     '/seq2seq/encoder_rnn.initial_cells (300,) trained',\n",
    "     '/seq2seq/encoder_rnn.initial_state (300,) trained',\n",
    "     '/seq2seq/linear.W (300, 1000) trained',\n",
    "     '/seq2seq/linear.b (1000,) trained',\n",
    "     '/seq2seq/main_lookup.W (1000, 300) trained',\n",
    "     '/seq2seq/translate_layer/linear_0.W (300, 300) trained',\n",
    "     '/seq2seq/translate_layer/linear_0.b (300,) trained']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Related work\n",
    "* lexical knowledge\n",
    "    * intensional definitions\n",
    "    * genus vs differentia ==> hypernymy\n",
    "* ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Experiments\n",
    "* Wordnet\n",
    "2. Similarity/relatedness benchmarks\n",
    "    * pair of words ranked by human annotators\n",
    "    * cosine similarity\n",
    "    * Spearman's correlation ($\\rho$) between benchmarks and model results\n",
    "1. Modèles évalués(Baselines ?):\n",
    "    * Hill's (see: [hill2015learning])\n",
    "    * AE (autoencoder)\n",
    "    * CPAE (... with consistency penalty)\n",
    "\n",
    "\n",
    "[hill2015learning]: https://arxiv.org/pdf/1504.00548.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dictionary only\n",
    "* Baselines\n",
    "    * GloVe ...\n",
    "    * word2vec [mikolov2013distributed]\n",
    "    * trained on : defined words + definitions\n",
    "        * (Note: C'est ce que j'avais commencé à regarder pour Enrich/WSA )\n",
    "* Hill's $\\equiv$ n/a\n",
    "* AE\n",
    "* CPAE ($\\lambda = 8$)\n",
    "1. CPAE-p ($\\lambda = 64$)\n",
    "    1. 'word2vec embeddings pre-trained on dictionary data'\n",
    "\n",
    "[mikolov2013distributed]: https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. (suite) Résultats:\n",
    "* Dans presque tous les cas, CPAE-p ($\\lambda = 64$) est significativement mieux que word2vec de base\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Optimisation de Pre-trained embeddings\n",
    "* Baseline\n",
    "    * word2vec\n",
    "        * trained on : Wikipedia dump\n",
    "* retrofitting [faruqui2014retrofitting]\n",
    "* dict2vec [tissier2017dict2vec]\n",
    "* Hill's \n",
    "* AE\n",
    "* CPAE ($\\lambda = 4$)\n",
    "\n",
    "\n",
    "[tissier2017dict2vec]: https://hal-ujm.archives-ouvertes.fr/ujm-01613953/file/emnlp2017.pdf\n",
    "[faruqui2014retrofitting]: https://arxiv.org/pdf/1411.4166.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations sur le code\n",
    "    \n",
    "* blocks et fuel\n",
    "    decorators .... \n",
    "* frameworks sont complexes\n",
    "    * Surtout theano\n",
    "* Theano == symbolic programming\n",
    "    * \"not your grand father's python ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions en suspens\n",
    "* Récurrence des définitions\n",
    "    * Ça se passe dans les tréfonds de fuel.stream ~~des 'op' de theano~~ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesure d'évaluation\n",
    "\n",
    "* Spearman's correlation : [Scipy]\n",
    "    \n",
    "    \n",
    "[Scipy]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lien avec la recherche actuelle sur les \"language models\"\n",
    "\n",
    "\n",
    "#### Universal Language Models (Pre-calculated models)\n",
    "1. BERT [devlin2018bert]\n",
    "1. ELMo [peters2018deep]\n",
    "1. ULMFiT [howard2018universal]\n",
    "\n",
    "\n",
    "#### Ma compréhension\n",
    "1. Différence avec les \"Word Embeddings\" traditionnels:\n",
    "    * Architecture du NN pour word2vec beaucoup plus simple.\n",
    "        * cf: skipgram, CBOW\n",
    "    * Utiliser un WE existant est relativement simple.\n",
    "        * Il suffit de calculer la distance entre des mots dans l'espace vectoriel\n",
    "        * Pour les ULMs c'est plus compliqué:\n",
    "            * cf: \"evaluate_embeddings.sh\"/\"bin/generate_embeddings.py\"\n",
    "\n",
    "\n",
    "2. Seq2seq: Variations sur le thème des \"autoencoders\" ??\n",
    "\n",
    "1. Character-based (vs word based pour les WE) ??  \n",
    "    * [seq2seq_keras]\n",
    "\n",
    "[devlin2018bert]: https://arxiv.org/pdf/1810.04805\n",
    "[howard2018universal]: https://arxiv.org/pdf/1801.06146\n",
    "[peters2018deep]: https://arxiv.org/pdf/1802.05365\n",
    "\n",
    "[seq2seq_keras]: https://github.com/Lykkos/enrich/blob/master/_notebooks/seq2seq_keras.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For local PDFs\n",
    "\n",
    "To allow links to local pdf files, need to patch the function\n",
    "\n",
    "    content_security_policy at line 651 in 'site-packages/notebook/base/handlers.py'\n",
    "##### cf: /Users/jmpoulin/miniconda2/lib/python2.7/site-packages/notebook/base/handlers.py\n",
    "\n",
    "\n",
    "from:\n",
    "\n",
    "    @property\n",
    "    def content_security_policy(self):\n",
    "        # In case we're serving HTML/SVG, confine any Javascript to a unique\n",
    "        # origin so it can't interact with the notebook server.\n",
    "        return super(AuthenticatedFileHandler, self).content_security_policy + \\\n",
    "                \"; sandbox allow-scripts\"\n",
    "\n",
    "to\n",
    "\n",
    "    @property\n",
    "    def content_security_policy(self):\n",
    "        # In case we're serving HTML/SVG, confine any Javascript to a unique\n",
    "        # origin so it can't interact with the notebook server.\n",
    "        return super(AuthenticatedFileHandler, self).content_security_policy + \\\n",
    "                \"\"\n",
    "\n",
    "See: [Content Security Policy], [Issue 3652]\n",
    "\n",
    "[Issue 3652]: https://github.com/jupyter/notebook/issues/3652\n",
    "[Content Security Policy]: https://content-security-policy.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py27co)",
   "language": "python",
   "name": "p27co"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
